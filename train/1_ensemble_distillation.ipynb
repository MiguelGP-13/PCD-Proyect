{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e3b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n",
    "                                     BatchNormalization, GlobalAveragePooling2D, LeakyReLU)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import datetime\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
    "from keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
    "from keras.applications.xception import preprocess_input as xception_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646015dd",
   "metadata": {},
   "source": [
    "### Ensemble y Distilación\n",
    "Se crea el ensemble con Voting classifier de los 4 modelos e intentar destilarlo por un único modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab2f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactiva todas las GPUs\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed5371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Functional name=cnn_mejorada, built=True>,\n",
       " <Functional name=functional_1, built=True>,\n",
       " <Functional name=functional_3, built=True>,\n",
       " <Functional name=functional_5, built=True>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carpeta donde tienes los modelos .keras\n",
    "folder = \"../models/classifier/\"\n",
    "\n",
    "# Cargar los modelos\n",
    "models_paths = [\"new_model_11_20_h12_29.keras\", \"vgg16_finetuned_11_12_12_38.keras\", \"resnet50_finetuned_11_12_13_48.keras\", \"inceptionv3_finetuned_11_12_15_16.keras\"]\n",
    "models_names = [\"scratch\", \"VGG19\", \"ResNet\", \"Xception\"]\n",
    "models = [tf.keras.models.load_model(os.path.join(folder, f)) for f in models_paths]\n",
    "preprocesses = [lambda x: x/255., vgg16_preprocess, resnet_preprocess, xception_preprocess]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d1410",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4455c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VotingTeacher(tf.keras.Model):\n",
    "#     def __init__(self, models, preprocess_fns=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             models: lista de modelos Keras ya cargados\n",
    "#             preprocess_fns: lista de funciones de preprocesado, una por modelo.\n",
    "#                             Cada función recibe X y devuelve X_preprocesado.\n",
    "#                             Si None, se usa identidad.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.models = models\n",
    "#         if preprocess_fns is None:\n",
    "#             preprocess_fns = [lambda x: x for _ in models]\n",
    "#         self.preprocess_fns = preprocess_fns\n",
    "\n",
    "#     def call(self, X, training=False):\n",
    "#         hard_votes = []\n",
    "#         soft_probs = []\n",
    "\n",
    "#         # Obtener predicciones de cada modelo\n",
    "#         for m, fn in zip(self.models, self.preprocess_fns):\n",
    "#             X_prep = fn(X)\n",
    "#             p = m(X_prep, training=training)  # salida (N,1)\n",
    "#             p = tf.cast(tf.reshape(p, (-1, 1)), tf.float32)\n",
    "#             hard_votes.append(tf.cast(p > 0.5, tf.int32))\n",
    "#             soft_probs.append(p)\n",
    "\n",
    "#         # Apilar resultados\n",
    "#         hard_votes = tf.concat(hard_votes, axis=1)   # (N, num_models)\n",
    "#         soft_probs = tf.concat(soft_probs, axis=1)   # (N, num_models)\n",
    "\n",
    "#         # Hard voting con desempate vía soft voting\n",
    "#         num_models = len(self.models)\n",
    "#         sum_votes = tf.reduce_sum(hard_votes, axis=1)  # (N,)\n",
    "\n",
    "#         def resolve_vote(i):\n",
    "#             if sum_votes[i] > num_models / 2:\n",
    "#                 return 1\n",
    "#             elif sum_votes[i] < num_models / 2:\n",
    "#                 return 0\n",
    "#             else:\n",
    "#                 # Empate → usar soft voting\n",
    "#                 return 1 if tf.reduce_mean(soft_probs[i]) > 0.5 else 0\n",
    "\n",
    "#         # Aplicar la regla a cada muestra\n",
    "#         majority_vote = tf.map_fn(lambda i: resolve_vote(i),\n",
    "#                                   tf.range(tf.shape(X)[0]),\n",
    "#                                   dtype=tf.int32)\n",
    "\n",
    "#         # Devolver como tensor (N,1)\n",
    "#         return tf.reshape(majority_vote, (-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587980f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingTeacher(tf.keras.Model):\n",
    "    def __init__(self, models, preprocess_fns=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models: lista de modelos Keras ya cargados\n",
    "            preprocess_fns: lista de funciones de preprocesado, una por modelo.\n",
    "                            Cada función recibe X y devuelve X_preprocesado.\n",
    "                            Si None, se usa identidad.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        if preprocess_fns is None:\n",
    "            print(\"Preprocess functions missing\")\n",
    "            preprocess_fns = [lambda x: x for _ in models]\n",
    "        self.preprocess_fns = preprocess_fns\n",
    "\n",
    "    def call(self, X, training=False):\n",
    "        preds = []\n",
    "        for m, fn in zip(self.models, self.preprocess_fns):\n",
    "            # aplicar preprocesado compatible con tf.Tensor\n",
    "            X_prep = fn(X)\n",
    "            p = m(X_prep, training=training)  # salida (N,1)\n",
    "            p = tf.cast(tf.reshape(p, (-1, 1)), tf.float32)\n",
    "            preds.append(p)\n",
    "        stacked = tf.stack(preds, axis=0)      # (num_models, N, 1)\n",
    "        avg_preds = tf.reduce_mean(stacked, axis=0)  # (N,1)\n",
    "        return avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d2292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9013 images belonging to 2 classes.\n",
      "Total imágenes usadas: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 13:10:37.599836: I external/local_xla/xla/service/service.cc:163] XLA service 0x79d4f000b650 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-20 13:10:37.599855: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): Host, Default Version\n",
      "2025-11-20 13:10:37.627517: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1763640637.840764   21547 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo scratch - Classification Report (10 batches):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9029    0.9080    0.9054       174\n",
      "         1.0     0.3600    0.3462    0.3529        26\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.6314    0.6271    0.6292       200\n",
      "weighted avg     0.8323    0.8350    0.8336       200\n",
      "\n",
      "\n",
      "Modelo VGG19 - Classification Report (10 batches):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9748    0.8908    0.9309       174\n",
      "         1.0     0.5366    0.8462    0.6567        26\n",
      "\n",
      "    accuracy                         0.8850       200\n",
      "   macro avg     0.7557    0.8685    0.7938       200\n",
      "weighted avg     0.9179    0.8850    0.8953       200\n",
      "\n",
      "\n",
      "Modelo ResNet - Classification Report (10 batches):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9274    0.9540    0.9405       174\n",
      "         1.0     0.6190    0.5000    0.5532        26\n",
      "\n",
      "    accuracy                         0.8950       200\n",
      "   macro avg     0.7732    0.7270    0.7469       200\n",
      "weighted avg     0.8873    0.8950    0.8902       200\n",
      "\n",
      "\n",
      "Modelo InceptionV3 - Classification Report (10 batches):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8756    0.9713    0.9210       174\n",
      "         1.0     0.2857    0.0769    0.1212        26\n",
      "\n",
      "    accuracy                         0.8550       200\n",
      "   macro avg     0.5807    0.5241    0.5211       200\n",
      "weighted avg     0.7990    0.8550    0.8170       200\n",
      "\n",
      "\n",
      "VotingTeacher - Classification Report (10 batches):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8700    1.0000    0.9305       174\n",
      "         1.0     0.0000    0.0000    0.0000        26\n",
      "\n",
      "    accuracy                         0.8700       200\n",
      "   macro avg     0.4350    0.5000    0.4652       200\n",
      "weighted avg     0.7569    0.8700    0.8095       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/python3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/miguel/python3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/miguel/python3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=60,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.12,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        shear_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"../data/processed/train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Extraer 10 batches del generador\n",
    "X_all, y_all = [], []\n",
    "for _ in range(50):\n",
    "    X_batch, y_batch = next(train_generator)\n",
    "    X_all.append(X_batch)\n",
    "    y_all.append(y_batch)\n",
    "\n",
    "X_all = np.vstack(X_all)\n",
    "y_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Total imágenes usadas: {X_all.shape[0]}\")\n",
    "\n",
    "# Evaluar cada modelo con su preprocesamiento y sacar classification_report\n",
    "for name, model, preprocess in zip(models_names, models, preprocesses):\n",
    "    X_prep = preprocess(X_all)\n",
    "    y_pred = model.predict(X_prep, verbose=0)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "    \n",
    "    print(f\"\\nModelo {name} - Classification Report (10 batches):\")\n",
    "    print(classification_report(y_all, y_pred_classes, digits=4))\n",
    "\n",
    "#Usar el VotingTeacher para predecir sobre tus datos\n",
    "y_pred = VotingTeacher(models, preprocesses)(X_all, training=False).numpy()\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"\\nVotingTeacher - Classification Report (10 batches):\")\n",
    "print(classification_report(y_all, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145ddcd",
   "metadata": {},
   "source": [
    "### Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6150bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Configure the distiller for binary classification.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation (e.g. BinaryAccuracy)\n",
    "            student_loss_fn: Loss between student predictions and ground-truth\n",
    "            distillation_loss_fn: Loss between soft student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data (puede traer sample_weight)\n",
    "        if isinstance(data, tuple):\n",
    "            if len(data) == 2:\n",
    "                x, y = data\n",
    "            elif len(data) == 3:\n",
    "                x, y, _ = data\n",
    "            else:\n",
    "                raise ValueError(f\"Formato inesperado en data: {len(data)} elementos\")\n",
    "        else:\n",
    "            # Si data es dict, usa las claves\n",
    "            x, y = data[\"x\"], data[\"y\"]\n",
    "\n",
    "        # Forward pass del teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            teacher_soft = tf.nn.sigmoid(teacher_predictions / self.temperature)\n",
    "            student_soft = tf.nn.sigmoid(student_predictions / self.temperature)\n",
    "\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(teacher_soft, student_soft)\n",
    "                * (self.temperature ** 2)\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y, student_predictions)\n",
    "\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n",
    "        return results\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update metrics\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y, y_prediction)\n",
    "\n",
    "        # Return dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d515989",
   "metadata": {},
   "source": [
    "#### Creamos el modelo Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d58ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador_binario(input_shape=(224,224,3), lr=1e-3):\n",
    "    entrada = Input(shape=input_shape, name='entrada_imagen')\n",
    "\n",
    "    # Bloque 1\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2(1e-4))(entrada)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    # Bloque 2\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    # Bloque 3\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    # Bloque 4 (extra para más capacidad)\n",
    "    x = Conv2D(256, (3,3), activation='relu', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    # Global pooling en lugar de Flatten (reduce parámetros)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Capa densa\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    salida = Dense(1, activation='sigmoid', name='salida_binaria')(x)\n",
    "\n",
    "    modelo = Model(inputs=entrada, outputs=salida, name='student')\n",
    "    modelo.compile(optimizer=Adam(learning_rate=lr),\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "    return modelo\n",
    "# Igual que el clasificador desde cero\n",
    "student = clasificador_binario()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d01c8",
   "metadata": {},
   "source": [
    "#### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f4ff5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/processed/train\"\n",
    "\n",
    "def get_generators(data_dir, preprocess_fn, target_size=(224, 224), batch_size=128, validation_split=0.15):\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_fn,\n",
    "        rotation_range=60,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.12,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        shear_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bf0a4",
   "metadata": {},
   "source": [
    "#### Instanciamos código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66f65169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7662 images belonging to 2 classes.\n",
      "Found 1351 images belonging to 2 classes.\n",
      "{0: 0.6213104119364256, 1: 2.560828877005348}\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator = get_generators(data_dir, lambda x: x)\n",
    "\n",
    "labels = train_generator.classes\n",
    "\n",
    "# Calculamos los pesos\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Lo convertimos en diccionario para Keras\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)\n",
    "\n",
    "teacher = VotingTeacher(models, preprocesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15051dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.2201 - distillation_loss: -1.5436 - loss: 0.1962 - student_loss: 2.7795 "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Distill teacher to student\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdistiller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate student on test dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m distiller\u001b[38;5;241m.\u001b[39mevaluate(val_generator)\n",
      "File \u001b[0;32m~/python3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[22], line 78\u001b[0m, in \u001b[0;36mDistiller.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Unpack data\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     x, y, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Compute predictions\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     y_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudent(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    student_loss_fn=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Ajustar nivel de logging de TensorFlow\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# Distill teacher to student\n",
    "distiller.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39926e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
