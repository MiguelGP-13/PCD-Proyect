{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13877733,"sourceType":"datasetVersion","datasetId":8656484}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport json\nimport pandas as pd\nimport plotly.express as px\nimport datetime\n\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Add, ReLU, Lambda\nfrom tensorflow.keras.models import Model,load_model\n\nAUTOTUNE = tf.data.AUTOTUNE\nIMG_SIZE = (128, 128)\nEMBED_DIM = 128\nBATCH_SIZE = 64\nEPOCHS = 150\nTEMPERATURE = 0.05\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:13.418244Z","iopub.execute_input":"2025-11-29T11:32:13.418519Z","iopub.status.idle":"2025-11-29T11:32:34.209881Z","shell.execute_reply.started":"2025-11-29T11:32:13.418494Z","shell.execute_reply":"2025-11-29T11:32:34.209275Z"}},"outputs":[{"name":"stderr","text":"2025-11-29 11:32:18.736410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764415938.978035      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764415939.043266      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"def contar_archivos_en_carpetas(directorio):\n    # Recorre todas las carpetas dentro del directorio\n    for carpeta in os.listdir(directorio):\n        ruta_carpeta = os.path.join(directorio, carpeta)\n        if os.path.isdir(ruta_carpeta):\n            # Cuenta solo archivos (no subcarpetas)\n            archivos = [f for f in os.listdir(ruta_carpeta) \n                        if os.path.isfile(os.path.join(ruta_carpeta, f))]\n            print(f\"Carpeta: {carpeta} -> {len(archivos)} archivos\")\n\n# Ejemplo de uso\ndirectorio_base = \"/kaggle/input/hampreprocessed/malignas_classes/train\"  # Cambia esto por tu ruta\ncontar_archivos_en_carpetas(directorio_base)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:34.210480Z","iopub.execute_input":"2025-11-29T11:32:34.210968Z","iopub.status.idle":"2025-11-29T11:32:38.636218Z","shell.execute_reply.started":"2025-11-29T11:32:34.210949Z","shell.execute_reply":"2025-11-29T11:32:38.635519Z"}},"outputs":[{"name":"stdout","text":"Carpeta: mel -> 948 archivos\nCarpeta: akiec -> 281 archivos\nCarpeta: bcc -> 442 archivos\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def contar_archivos_por_clase(directorio_base):\n    clases_totales = {}  # acumulador por clase\n\n    for conjunto in [\"train\", \"test\"]:\n        ruta_conjunto = os.path.join(directorio_base, conjunto)\n        if not os.path.exists(ruta_conjunto):\n            print(f\"No existe la carpeta: {ruta_conjunto}\")\n            continue\n\n        print(f\"\\nConjunto: {conjunto}\")\n        for carpeta in os.listdir(ruta_conjunto):\n            ruta_carpeta = os.path.join(ruta_conjunto, carpeta)\n            if os.path.isdir(ruta_carpeta):\n                archivos = [f for f in os.listdir(ruta_carpeta) \n                            if os.path.isfile(os.path.join(ruta_carpeta, f))]\n                cantidad = len(archivos)\n                print(f\"  Carpeta: {carpeta} -> {cantidad} archivos\")\n\n                # acumular por clase\n                if carpeta not in clases_totales:\n                    clases_totales[carpeta] = 0\n                clases_totales[carpeta] += cantidad\n\n    # Mostrar suma total por clase\n    print(\"\\nSuma total por clase (train + test):\")\n    for clase, total in clases_totales.items():\n        print(f\"  {clase} -> {total} archivos\")\n\n# Ejemplo de uso\ndirectorio_base = \"/kaggle/input/hampreprocessed/malignas_classes\"  # Ruta base que contiene train y test\ncontar_archivos_por_clase(directorio_base)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T13:50:13.920006Z","iopub.execute_input":"2025-11-26T13:50:13.920587Z","iopub.status.idle":"2025-11-26T13:50:13.986034Z","shell.execute_reply.started":"2025-11-26T13:50:13.920561Z","shell.execute_reply":"2025-11-26T13:50:13.985453Z"}},"outputs":[{"name":"stdout","text":"\nConjunto: train\n  Carpeta: benignas -> 6855 archivos\n  Carpeta: malignas -> 1662 archivos\n\nConjunto: test\n  Carpeta: benignas -> 807 archivos\n  Carpeta: malignas -> 195 archivos\n\nSuma total por clase (train + test):\n  benignas -> 7662 archivos\n  malignas -> 1857 archivos\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Importación de datos","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/hampreprocessed/malignas_classes/train\"\nval_dir = \"/kaggle/input/hampreprocessed/malignas_classes/val\"\n\ndef get_generators(data_dir, val_dir, preprocess_fn, target_size=(224, 224), batch_size=256):\n    datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_fn,\n        rotation_range=60,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        zoom_range=0.12,\n        brightness_range=[0.8, 1.2],\n        shear_range=0.2,\n        vertical_flip=True,\n        horizontal_flip=True\n    )\n\n    train_generator = datagen.flow_from_directory(\n        data_dir,\n        target_size=target_size,\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=True\n    )\n\n    val_generator = ImageDataGenerator(preprocessing_function=preprocess_fn).flow_from_directory(\n        val_dir,\n        target_size=target_size,\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=False\n    )\n\n    return train_generator, val_generator\n\ntrain_generator, val_generator = get_generators(data_dir, val_dir, lambda x: x)\nprint(pd.Series(val_generator.classes).value_counts())\nprint(pd.Series(train_generator.classes).value_counts())\n\nlabels = train_generator.classes  \n\n# Calculamos los pesos\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(labels),\n    y=labels\n)\n\n# Lo convertimos en diccionario para Keras\nclass_weights = dict(enumerate(class_weights))\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:38.637531Z","iopub.execute_input":"2025-11-29T11:32:38.637779Z","iopub.status.idle":"2025-11-29T11:32:39.216243Z","shell.execute_reply.started":"2025-11-29T11:32:38.637757Z","shell.execute_reply":"2025-11-29T11:32:39.215615Z"}},"outputs":[{"name":"stdout","text":"Found 1671 images belonging to 3 classes.\nFound 88 images belonging to 3 classes.\n2    54\n1    19\n0    15\nName: count, dtype: int64\n2    948\n1    442\n0    281\nName: count, dtype: int64\n{0: 1.9822064056939501, 1: 1.260180995475113, 2: 0.5875527426160337}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_generator, val_generator = get_generators(data_dir, val_dir, lambda x: x/255., target_size=IMG_SIZE, batch_size=BATCH_SIZE)\nnum_classes = len(train_generator.class_indices)\nclass_names = list(train_generator.class_indices.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:39.216943Z","iopub.execute_input":"2025-11-29T11:32:39.217155Z","iopub.status.idle":"2025-11-29T11:32:39.354395Z","shell.execute_reply.started":"2025-11-29T11:32:39.217138Z","shell.execute_reply":"2025-11-29T11:32:39.353647Z"}},"outputs":[{"name":"stdout","text":"Found 1671 images belonging to 3 classes.\nFound 88 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Modelo generador de embeddings","metadata":{}},{"cell_type":"code","source":"def create_transfer_model(base_model_fn, input_shape=(224,224,3), n_classes=1, dropout=0.2, trainable_layers=0):\n    base = base_model_fn(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=input_shape\n    )\n\n    # Congelamos todas las capas primero\n    base.trainable = False\n\n    # Si se especifican capas entrenables, las activamos desde el final\n    if trainable_layers > 0:\n        for layer in base.layers[-trainable_layers:]:\n            layer.trainable = True\n\n    x = GlobalAveragePooling2D()(base.output)\n    x = Dense(128, activation=\"relu\")(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    output = Dense(n_classes, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=base.input, outputs=output)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:39.355219Z","iopub.execute_input":"2025-11-29T11:32:39.355552Z","iopub.status.idle":"2025-11-29T11:32:39.361365Z","shell.execute_reply.started":"2025-11-29T11:32:39.355528Z","shell.execute_reply":"2025-11-29T11:32:39.360750Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def contrastive_encoder(input_shape=(IMG_SIZE[0],IMG_SIZE[1],3), embedding_dim=EMBED_DIM):\n    inputs = Input(shape=input_shape)\n\n    # Bloque 1\n    x = Conv2D(64, 3, padding='same', use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(64, 3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    s = Conv2D(64, 1, padding='same', use_bias=False)(inputs)\n    s = BatchNormalization()(s)\n    x = Add()([x, s])\n    x = ReLU()(x)\n    x = MaxPooling2D()(x)\n\n    # Bloque 2\n    y = Conv2D(128, 3, padding='same', use_bias=False)(x)\n    y = BatchNormalization()(y)\n    y = ReLU()(y)\n    y = Conv2D(128, 3, padding='same', use_bias=False)(y)\n    y = BatchNormalization()(y)\n    s2 = Conv2D(128, 1, padding='same', use_bias=False)(x)\n    s2 = BatchNormalization()(s2)\n    y = Add()([y, s2])\n    y = ReLU()(y)\n    y = MaxPooling2D()(y)\n\n    # Bloque 3\n    z = Conv2D(256, 3, padding='same', use_bias=False)(y)\n    z = BatchNormalization()(z)\n    z = ReLU()(z)\n    z = Conv2D(256, 3, padding='same', use_bias=False)(z)\n    z = BatchNormalization()(z)\n    s3 = Conv2D(256, 1, padding='same', use_bias=False)(y)\n    s3 = BatchNormalization()(s3)\n    z = Add()([z, s3])\n    z = ReLU()(z)\n\n    z = GlobalAveragePooling2D()(z)\n    z = Dense(512, activation='relu')(z)\n    z = BatchNormalization()(z)\n\n    # Proyección (cabeza contrastiva)timestamps\n    p = Dense(embedding_dim, activation='relu')(z)\n    p = Dense(embedding_dim)(p)\n    outputs = Lambda(\n    lambda t: tf.math.l2_normalize(t, axis=1),\n    name=\"proj_norm\",\n    output_shape=(embedding_dim,))(p)\n\n\n    return Model(inputs, outputs, name=\"ContrastiveEncoder\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:39.362041Z","iopub.execute_input":"2025-11-29T11:32:39.362327Z","iopub.status.idle":"2025-11-29T11:32:39.380008Z","shell.execute_reply.started":"2025-11-29T11:32:39.362310Z","shell.execute_reply":"2025-11-29T11:32:39.379240Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SupConLoss(tf.keras.losses.Loss):\n    def __init__(self, temperature=0.1, name=\"supcon\"):\n        super().__init__(name=name)\n        self.temperature = temperature\n\n    def call(self, y_true, features):\n        \"\"\"\n        SupConLoss implementation.\n        Args:\n            y_true: [batch] integer class labels (not one-hot).\n            features: [batch, dim] embeddings.\n        \"\"\"\n        # Normalize embeddings\n        features = tf.math.l2_normalize(features, axis=1)\n        batch_size = tf.shape(features)[0]\n\n        # Similarity matrix\n        sim = tf.matmul(features, features, transpose_b=True)  # [B, B]\n        sim = sim / self.temperature\n\n        # Ensure labels are integers, not one-hot\n        if y_true.shape.ndims > 1 and y_true.shape[-1] > 1:\n            y_true = tf.argmax(y_true, axis=-1)\n\n        labels = tf.reshape(y_true, [-1, 1])  # [B, 1]\n        mask = tf.equal(labels, tf.transpose(labels))  # [B, B]\n        mask = tf.cast(mask, tf.float32)\n\n        # Remove self-contrast\n        eye = tf.eye(batch_size, dtype=tf.float32)\n        logits_mask = tf.ones_like(mask) - eye\n        mask = mask * logits_mask\n\n        # Log-softmax denominator excluding self\n        sim_max = tf.reduce_max(sim, axis=1, keepdims=True)\n        sim = sim - sim_max\n        exp_sim = tf.exp(sim) * logits_mask\n        denom = tf.reduce_sum(exp_sim, axis=1, keepdims=True) + 1e-9\n        log_prob = sim - tf.math.log(denom)\n\n        # Average log-prob of positives per anchor\n        pos_count = tf.reduce_sum(mask, axis=1) + 1e-9\n        mean_log_pos = tf.reduce_sum(mask * log_prob, axis=1) / pos_count\n\n        loss = -tf.reduce_mean(mean_log_pos)\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:39.381056Z","iopub.execute_input":"2025-11-29T11:32:39.381292Z","iopub.status.idle":"2025-11-29T11:32:39.434576Z","shell.execute_reply.started":"2025-11-29T11:32:39.381275Z","shell.execute_reply":"2025-11-29T11:32:39.433920Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Entrenar representaciones","metadata":{}},{"cell_type":"code","source":"def evaluate_embeddings(model, train_generator, val_generator, k=3, train_steps=50, val_steps=50):\n    \"\"\"Entrena KNN y NearestCentroid con embeddings de train y evalúa en val.\"\"\"\n    # --- Embeddings de train ---\n    train_embeds, train_labels = [], []\n    for _ in range(train_steps):\n        images, labels = next(train_generator)\n        embeds = model(images, training=False).numpy()\n        train_embeds.append(embeds)\n        train_labels.append(np.argmax(labels, axis=1))  # convertir one-hot a entero\n    X_train = np.concatenate(train_embeds, axis=0)\n    y_train = np.concatenate(train_labels, axis=0)\n\n    # --- Embeddings de val ---\n    val_embeds, val_labels = [], []\n    for _ in range(val_steps):\n        images, labels = next(val_generator)\n        embeds = model(images, training=False).numpy()\n        val_embeds.append(embeds)\n        val_labels.append(np.argmax(labels, axis=1))\n    X_val = np.concatenate(val_embeds, axis=0)\n    y_val = np.concatenate(val_labels, axis=0)\n\n    # --- KNN ---\n    knn = KNeighborsClassifier(n_neighbors=k, metric=\"cosine\", weights=\"distance\")\n    knn.fit(X_train, y_train)\n    y_pred_knn = knn.predict(X_val)\n    acc_knn = accuracy_score(y_val, y_pred_knn)\n\n    # --- Nearest Centroid ---\n    centroid = NearestCentroid(metric=\"cosine\")\n    centroid.fit(X_train, y_train)\n    y_pred_centroid = centroid.predict(X_val)\n    acc_centroid = accuracy_score(y_val, y_pred_centroid)\n\n    print(f\"k-NN Acc: {acc_knn:.4f}\")\n    print(f\"Nearest Centroid Acc: {acc_centroid:.4f}\")\n\n    return acc_knn, acc_centroid\n\n\ndef train_supcon(model, train_generator, val_generator, loss_fn, optimizer, epochs=50, accumulate_steps=2):\n    steps_per_epoch = train_generator.samples // train_generator.batch_size\n    validation_steps = val_generator.samples // val_generator.batch_size\n\n    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n    val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n\n    for epoch in range(epochs):\n        train_loss.reset_state()\n        val_loss.reset_state()\n\n        # Training\n        accum_grads = [tf.zeros_like(var) for var in model.trainable_variables]\n        step_count = 0\n\n        for _ in range(steps_per_epoch):\n            images, labels = next(train_generator)\n\n            with tf.GradientTape() as tape:\n                embeddings = model(images, training=True)\n                loss = loss_fn(labels, embeddings)\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            accum_grads = [accum + grad for accum, grad in zip(accum_grads, grads)]\n            step_count += 1\n            train_loss.update_state(loss)\n\n            # Apply gradients every `accumulate_steps`\n            if step_count % accumulate_steps == 0:\n                mean_grads = [accum / accumulate_steps for accum in accum_grads]\n                optimizer.apply_gradients(zip(mean_grads, model.trainable_variables))\n                accum_grads = [tf.zeros_like(var) for var in model.trainable_variables]\n\n        # Validation\n        for _ in range(validation_steps):\n            images, labels = next(val_generator)\n            embeddings = model(images, training=False)\n            loss = loss_fn(labels, embeddings)\n            val_loss.update_state(loss)\n\n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.result():.4f} - Val Loss: {val_loss.result():.4f}\")\n\n        # Evaluation every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            acc_knn, acc_centroid = evaluate_embeddings(\n                model, train_generator, val_generator,\n                k=3, train_steps=steps_per_epoch, val_steps=validation_steps\n            )\n            print(f\"k-NN Acc: {acc_knn:.4f}\")\n            print(f\"Nearest Centroid Acc: {acc_centroid:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:32:39.437432Z","iopub.execute_input":"2025-11-29T11:32:39.438017Z","iopub.status.idle":"2025-11-29T11:32:39.463676Z","shell.execute_reply.started":"2025-11-29T11:32:39.437984Z","shell.execute_reply":"2025-11-29T11:32:39.462814Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"encoder = contrastive_encoder(embedding_dim=EMBED_DIM)\nencoder.trainable = True\nloss_fn = SupConLoss(temperature=TEMPERATURE)\noptimizer = Adam(learning_rate=8e-4)\ntrain_supcon(encoder, train_generator, val_generator, loss_fn, optimizer, epochs=5)\n# Current timestamp\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n\nencoder.save(f\"encoder_{timestamp}.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:23:59.130403Z","iopub.execute_input":"2025-11-26T14:23:59.130967Z","iopub.status.idle":"2025-11-26T14:26:19.895934Z","shell.execute_reply.started":"2025-11-26T14:23:59.130940Z","shell.execute_reply":"2025-11-26T14:26:19.895137Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5 - Train Loss: 3.7674 - Val Loss: 4.2446\nEpoch 2/5 - Train Loss: 3.7599 - Val Loss: 3.3043\nEpoch 3/5 - Train Loss: 3.7808 - Val Loss: 3.8724\nEpoch 4/5 - Train Loss: 3.7725 - Val Loss: 3.3122\nEpoch 5/5 - Train Loss: 3.7239 - Val Loss: 4.0184\nk-NN Acc: 1.0000\nNearest Centroid Acc: 1.0000\nk-NN Acc: 1.0000\nNearest Centroid Acc: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_nearest_centroid.py:179: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from keras.applications.vgg16 import preprocess_input as vgg16_preprocess\ntrain_generator, val_generator = get_generators(data_dir,val_dir, vgg16_preprocess)\nvgg_encoder = create_transfer_model(VGG16, trainable_layers= 4)\nencoder.trainable = True\nloss_fn = SupConLoss(temperature=TEMPERATURE)\noptimizer = Adam(learning_rate=8e-4)\ntrain_supcon(vgg_encoder, train_generator, val_generator, loss_fn, optimizer, epochs=25)\n\n# Guardar\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_%H:%M\")\nvgg_encoder.save(os.path.join(save_dir, f\"vgg16_encoder_{timestamp}.keras\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:36:42.006670Z","iopub.execute_input":"2025-11-29T11:36:42.006976Z","iopub.status.idle":"2025-11-29T11:37:01.102505Z","shell.execute_reply.started":"2025-11-29T11:36:42.006956Z","shell.execute_reply":"2025-11-29T11:37:01.101501Z"}},"outputs":[{"name":"stdout","text":"Found 1671 images belonging to 3 classes.\nFound 88 images belonging to 3 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1011283216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSupConLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEMPERATURE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_supcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Guardar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/459893304.py\u001b[0m in \u001b[0;36mtrain_supcon\u001b[0;34m(model, train_generator, val_generator, loss_fn, optimizer, epochs, accumulate_steps)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1m{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[256,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(256, 224, 224, 64), dtype=float32)"],"ename":"ResourceExhaustedError","evalue":"Exception encountered when calling Conv2D.call().\n\n\u001b[1m{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[256,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(256, 224, 224, 64), dtype=float32)","output_type":"error"}],"execution_count":15},{"cell_type":"markdown","source":"## Calcular centroides de clases","metadata":{}},{"cell_type":"code","source":"def compute_centroids(encoder, generator):\n    \"\"\"\n    Calcula centroides de clase a partir de un generator de Keras.\n    Devuelve un dict {class_index: centroid_vector}.\n    \"\"\"\n    embeds, labels = [], []\n    for i in range(len(generator)):\n        x_batch, y_batch = generator[i]\n        e = encoder.predict(x_batch, verbose=0)\n        e = normalize(e)  # normalizar embeddings fila a fila\n        embeds.append(e)\n        labels.append(np.argmax(y_batch, axis=1))  # convertir one-hot a entero\n    # print(pd.Series(labels).value_counts())\n\n    embeds = np.concatenate(embeds)\n    labels = np.concatenate(labels)\n\n    centroids = {}\n    for c in np.unique(labels):\n        class_embeds = embeds[labels == c]\n        centroid = class_embeds.mean(axis=0)\n        centroid = centroid / np.linalg.norm(centroid)  # normalizar centroide\n        centroids[int(c)] = centroid.tolist()  # convertir a lista para JSON\n\n    return centroids\n\ndef save_centroids(centroids, filename=None):\n    if not filename:\n        timestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n        filename = f\"centroids_{timestamp}.json\"\n    with open(filename, \"w\") as f:\n        json.dump(centroids, f)\n\ndef load_centroids(filename=\"centroids.json\"):\n    with open(filename, \"r\") as f:\n        centroids = json.load(f)\n    # convertir a numpy arrays\n    centroids = {int(k): np.array(v) for k, v in centroids.items()}\n    return centroids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:27:20.368594Z","iopub.execute_input":"2025-11-26T14:27:20.369434Z","iopub.status.idle":"2025-11-26T14:27:20.376819Z","shell.execute_reply.started":"2025-11-26T14:27:20.369408Z","shell.execute_reply":"2025-11-26T14:27:20.375969Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"centroids = compute_centroids(encoder, train_generator)\nsave_centroids(centroids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:27:24.126704Z","iopub.execute_input":"2025-11-26T14:27:24.127264Z","iopub.status.idle":"2025-11-26T14:27:51.756972Z","shell.execute_reply.started":"2025-11-26T14:27:24.127241Z","shell.execute_reply":"2025-11-26T14:27:51.756326Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1764167245.090675     770 service.cc:148] XLA service 0x7b5600005d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1764167245.091266     770 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1764167248.198486     770 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Evaluar","metadata":{}},{"cell_type":"code","source":"def predict_class(encoder, x, centroids, probs=False):\n    \"\"\"\n    Predice la clase de una sola imagen x usando centroides.\n    \"\"\"\n    e = encoder.predict(np.expand_dims(x, axis=0), verbose=0)\n    e = normalize(e)  # normalizar embedding\n    sims = {c: np.dot(e, centroids[c]) for c in centroids}\n    if probs:\n        return sims\n    return max(sims, key=sims.get)  # clase con mayor similitud\n\ndef evaluate_accuracy(encoder, val_generator, centroids):\n    \"\"\"\n    Calcula el accuracy del val_generator usando centroides.\n    \"\"\"\n    y_true, y_pred = [], []\n\n    for i in range(len(val_generator)):\n        x_batch, y_batch = val_generator[i]\n        labels = np.argmax(y_batch, axis=1)  # convertir one-hot a enteros\n\n        for j in range(len(x_batch)):\n            pred = predict_class(encoder, x_batch[j], centroids)\n            y_true.append(labels[j])\n            y_pred.append(pred)\n\n    acc = accuracy_score(y_true, y_pred)\n    print(f\"Accuracy en val_generator: {acc:.4f}\")\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:27:51.758178Z","iopub.execute_input":"2025-11-26T14:27:51.758754Z","iopub.status.idle":"2025-11-26T14:27:51.764811Z","shell.execute_reply.started":"2025-11-26T14:27:51.758733Z","shell.execute_reply":"2025-11-26T14:27:51.764120Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"evaluate_accuracy(encoder, val_generator, centroids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T14:27:51.765450Z","iopub.execute_input":"2025-11-26T14:27:51.765780Z","iopub.status.idle":"2025-11-26T14:29:51.769369Z","shell.execute_reply.started":"2025-11-26T14:27:51.765763Z","shell.execute_reply":"2025-11-26T14:29:51.768582Z"}},"outputs":[{"name":"stdout","text":"Accuracy en val_generator: 0.6373\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.6373429084380611"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## KNN as classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef train_knn(encoder, train_generator, k=10):\n    \"\"\"\n    Entrena un KNN sobre los embeddings del train_generator.\n    Devuelve el clasificador entrenado.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(train_generator)):\n        x_batch, y_batch = train_generator[i]\n        e = encoder.predict(x_batch, verbose=0)\n        e = e / np.linalg.norm(e, axis=1, keepdims=True)  # normalizar embeddings\n        X.append(e)\n        y.append(np.argmax(y_batch, axis=1))\n\n    X = np.concatenate(X)\n    y = np.concatenate(y)\n\n    knn = KNeighborsClassifier(n_neighbors=k, metric=\"cosine\")\n    knn.fit(X, y)\n    return knn\n\ndef evaluate_knn(encoder, val_generator, knn):\n    \"\"\"\n    Evalúa un KNN entrenado sobre el val_generator.\n    \"\"\"\n    X_val, y_val = [], []\n    for i in range(len(val_generator)):\n        x_batch, y_batch = val_generator[i]\n        e = encoder.predict(x_batch, verbose=0)\n        e = e / np.linalg.norm(e, axis=1, keepdims=True)\n        X_val.append(e)\n        y_val.append(np.argmax(y_batch, axis=1))\n\n    X_val = np.concatenate(X_val)\n    y_val = np.concatenate(y_val)\n\n    y_pred = knn.predict(X_val)\n    acc = accuracy_score(y_val, y_pred)\n    print(f\"Accuracy en val_generator con KNN: {acc:.4f}\")\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:33:22.814929Z","iopub.execute_input":"2025-11-29T11:33:22.815218Z","iopub.status.idle":"2025-11-29T11:33:22.822777Z","shell.execute_reply.started":"2025-11-29T11:33:22.815194Z","shell.execute_reply":"2025-11-29T11:33:22.822096Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"encoder = contrastive_encoder(\n    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n    embedding_dim=EMBED_DIM\n)\n\n# Cargar solo los pesos entrenados\nencoder.load_weights(\"/kaggle/working/encoder_finetuned_11_26_h15_34.keras\")\n\nknn = train_knn(encoder, train_generator)\nimport pickle\n\n# Guardar el modelo entrenado en un archivo\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\nwith open(f\"knn_model_{timestamp}.pkl\", \"wb\") as f:\n    pickle.dump(knn, f)\nprint(\"Entrenado\")\nevaluate_knn(encoder, val_generator, knn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:35:09.921129Z","iopub.execute_input":"2025-11-29T11:35:09.921917Z","iopub.status.idle":"2025-11-29T11:35:34.083036Z","shell.execute_reply.started":"2025-11-29T11:35:09.921891Z","shell.execute_reply":"2025-11-29T11:35:34.082430Z"}},"outputs":[{"name":"stdout","text":"Entrenado\nAccuracy en val_generator con KNN: 0.7841\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0.7840909090909091"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"encoder = contrastive_encoder(\n    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n    embedding_dim=EMBED_DIM\n)\n\n# Cargar solo los pesos entrenados\nencoder.load_weights(\"/kaggle/working/encoder_11_26_h14_27.keras\")\nknn = train_knn(encoder, train_generator, k=3)\nimport pickle\n\n# Guardar el modelo entrenado en un archivo\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\nwith open(f\"knn_model_{timestamp}.pkl\", \"wb\") as f:\n    pickle.dump(knn, f)\nprint(\"Entrenado\")\nevaluate_knn(encoder, val_generator, knn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:36:19.915459Z","iopub.execute_input":"2025-11-29T11:36:19.915757Z","iopub.status.idle":"2025-11-29T11:36:42.005064Z","shell.execute_reply.started":"2025-11-29T11:36:19.915730Z","shell.execute_reply":"2025-11-29T11:36:42.004399Z"}},"outputs":[{"name":"stdout","text":"Entrenado\nAccuracy en val_generator con KNN: 0.8295\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0.8295454545454546"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"encoder.trainable = False\nx = encoder.output\nclf = Dense(num_classes, activation=\"softmax\")(x)\nclassifier = Model(encoder.input, clf)\nclassifier.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:14:14.550510Z","iopub.execute_input":"2025-11-26T15:14:14.551271Z","iopub.status.idle":"2025-11-26T15:14:14.577047Z","shell.execute_reply.started":"2025-11-26T15:14:14.551244Z","shell.execute_reply":"2025-11-26T15:14:14.576356Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"labels = train_generator.classes\nprint(pd.Series(labels).value_counts())\nprint(num_classes)\nclass_weights = dict(enumerate(compute_class_weight(\n    class_weight=\"balanced\",\n    classes=np.unique(labels),\n    y=labels\n)))\n\nclassifier.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nencoder.trainable = True\nclassifier.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=10,\n    class_weight=class_weights\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:30:38.729661Z","iopub.execute_input":"2025-11-26T15:30:38.730228Z","iopub.status.idle":"2025-11-26T15:33:44.759443Z","shell.execute_reply.started":"2025-11-26T15:30:38.730208Z","shell.execute_reply":"2025-11-26T15:33:44.758879Z"}},"outputs":[{"name":"stdout","text":"2    948\n1    442\n0    281\nName: count, dtype: int64\n3\nEpoch 1/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 756ms/step - accuracy: 0.8555 - loss: 0.3975 - val_accuracy: 0.8523 - val_loss: 0.3850\nEpoch 2/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 638ms/step - accuracy: 0.8616 - loss: 0.4258 - val_accuracy: 0.8523 - val_loss: 0.3888\nEpoch 3/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 637ms/step - accuracy: 0.8578 - loss: 0.4094 - val_accuracy: 0.8523 - val_loss: 0.3921\nEpoch 4/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 653ms/step - accuracy: 0.8530 - loss: 0.3950 - val_accuracy: 0.8295 - val_loss: 0.4187\nEpoch 5/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 651ms/step - accuracy: 0.8671 - loss: 0.4082 - val_accuracy: 0.8409 - val_loss: 0.3976\nEpoch 6/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 656ms/step - accuracy: 0.8726 - loss: 0.3902 - val_accuracy: 0.8409 - val_loss: 0.3950\nEpoch 7/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 628ms/step - accuracy: 0.8829 - loss: 0.3808 - val_accuracy: 0.8182 - val_loss: 0.4450\nEpoch 8/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 628ms/step - accuracy: 0.8878 - loss: 0.3756 - val_accuracy: 0.8182 - val_loss: 0.4339\nEpoch 9/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 639ms/step - accuracy: 0.8848 - loss: 0.3675 - val_accuracy: 0.8182 - val_loss: 0.4260\nEpoch 10/10\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 627ms/step - accuracy: 0.8828 - loss: 0.3632 - val_accuracy: 0.8182 - val_loss: 0.4345\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d23903d64d0>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Current timestamp\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n\nencoder.save(f\"encoder_finetuned_{timestamp}.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:34:21.407360Z","iopub.execute_input":"2025-11-26T15:34:21.407763Z","iopub.status.idle":"2025-11-26T15:34:21.520531Z","shell.execute_reply.started":"2025-11-26T15:34:21.407739Z","shell.execute_reply":"2025-11-26T15:34:21.519727Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Current timestamp\ntimestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n\nclassifier.save(f\"classifier_{timestamp}.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:34:29.648156Z","iopub.execute_input":"2025-11-26T15:34:29.648848Z","iopub.status.idle":"2025-11-26T15:34:29.835999Z","shell.execute_reply.started":"2025-11-26T15:34:29.648823Z","shell.execute_reply":"2025-11-26T15:34:29.835418Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def visualize_embeddings_3d(model, val_generator, class_names, method=\"tsne\"):\n    # 1. Calcular cuántos pasos tiene la validación\n    validation_steps = val_generator.samples // val_generator.batch_size\n\n    embs, labs = [], []\n    for _ in range(validation_steps):\n        images, labels = next(val_generator)\n        e = model(images, training=False).numpy()\n        embs.append(e)\n        labs.append(labels)\n\n    X = np.concatenate(embs, axis=0)\n    y = np.concatenate(labs, axis=0)\n\n    # 2. Reducir a 3D\n    if method == \"tsne\":\n        reducer = TSNE(n_components=3, perplexity=30, learning_rate=200, random_state=42)\n    else:\n        reducer = PCA(n_components=3)\n    X_reduced = reducer.fit_transform(X)\n\n    # 3. Visualizar con Plotly\n    fig = px.scatter_3d(\n        x=X_reduced[:,0], y=X_reduced[:,1], z=X_reduced[:,2],\n        color=[class_names[i] for i in y],\n        title=f\"Embeddings en 3D ({method.upper()})\",\n        opacity=0.7\n    )\n    fig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:39:09.137123Z","iopub.execute_input":"2025-11-26T15:39:09.137402Z","iopub.status.idle":"2025-11-26T15:39:09.143235Z","shell.execute_reply.started":"2025-11-26T15:39:09.137380Z","shell.execute_reply":"2025-11-26T15:39:09.142665Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"visualize_embeddings_3d(encoder, val_generator, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:39:01.704109Z","iopub.status.idle":"2025-11-26T15:39:01.704539Z","shell.execute_reply.started":"2025-11-26T15:39:01.704387Z","shell.execute_reply":"2025-11-26T15:39:01.704403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def euclidean_distance(vects):\n    \"\"\"Find the Euclidean distance between two vectors.\n\n    Arguments:\n        vects: List containing two tensors of same length.\n\n    Returns:\n        Tensor containing euclidean distance\n        (as floating point value) between vectors.\n    \"\"\"\n\n    x, y = vects\n    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n\n\ndef build_siamese_network(encoder, input_shape):\n    input_a = Input(shape=input_shape)\n    input_b = Input(shape=input_shape)\n\n    encoded_a = encoder(input_a)\n    encoded_b = encoder(input_b)\n\n    # Distancia euclídea entre embeddings\n    distance = Lambda(euclidean_distance)([encoded_a, encoded_b])\n\n    # Una neurona con sigmoide decide si son similares\n    outputs = Dense(1, activation=\"sigmoid\")(distance)\n\n    siamese_net = Model([input_a, input_b], outputs)\n    return siamese_net","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:39:16.726972Z","iopub.execute_input":"2025-11-26T15:39:16.727239Z","iopub.status.idle":"2025-11-26T15:39:16.732895Z","shell.execute_reply.started":"2025-11-26T15:39:16.727220Z","shell.execute_reply":"2025-11-26T15:39:16.732103Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"encoder = contrastive_encoder(\n    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n    embedding_dim=EMBED_DIM\n)\n\n# Cargar solo los pesos entrenados\nencoder.load_weights(\"/kaggle/working/encoder_11_26_h14_27.keras\")\n\nencoder.trainable=True\nsiamese_model = build_siamese_network(encoder, (IMG_SIZE[0], IMG_SIZE[0], 3))\nsiamese_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(1e-5), metrics=[\"accuracy\"])\nsiamese_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:42:54.634300Z","iopub.execute_input":"2025-11-26T15:42:54.634607Z","iopub.status.idle":"2025-11-26T15:42:55.027606Z","shell.execute_reply.started":"2025-11-26T15:42:54.634583Z","shell.execute_reply":"2025-11-26T15:42:55.026875Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ ContrastiveEncoder  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m1,406,848\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ ContrastiveEncod… │\n│                     │                   │            │ ContrastiveEncod… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ ContrastiveEncoder  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,406,848</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ContrastiveEncod… │\n│                     │                   │            │ ContrastiveEncod… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,406,850\u001b[0m (5.37 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,406,850</span> (5.37 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,403,138\u001b[0m (5.35 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,403,138</span> (5.35 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,712\u001b[0m (14.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> (14.50 KB)\n</pre>\n"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import random\ndef make_pairs_from_generator(generator):\n    \"\"\"\n    Crea pares de imágenes (positivos y negativos) a partir de un generator de Keras.\n    \n    Arguments:\n        generator: un ImageDataGenerator.flow_from_directory u otro generator que devuelva (x_batch, y_batch).\n    \n    Returns:\n        pairs: numpy array de shape (2*N, 2, H, W, C)\n        labels: numpy array binario de shape (2*N,)\n    \"\"\"\n    # --- 1. Extraer todas las imágenes y etiquetas del generator ---\n    all_images, all_labels = [], []\n    for i in range(len(generator)):\n        x_batch, y_batch = generator[i]\n        all_images.append(x_batch)\n        all_labels.append(np.argmax(y_batch, axis=1))  # convertir one-hot a entero\n    \n    x = np.concatenate(all_images, axis=0)\n    y = np.concatenate(all_labels, axis=0)\n\n    # --- 2. Crear índices por clase ---\n    num_classes = np.max(y) + 1\n    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n\n    pairs = []\n    labels = []\n\n    # --- 3. Generar pares ---\n    for idx1 in range(len(x)):\n        x1 = x[idx1]\n        label1 = y[idx1]\n\n        # Par positivo (misma clase)\n        idx2 = random.choice(digit_indices[label1])\n        x2 = x[idx2]\n        pairs.append([x1, x2])\n        labels.append(1)  # aquí 1 = misma clase\n\n        # Par negativo (clase distinta)\n        label2 = random.randint(0, num_classes - 1)\n        while label2 == label1:\n            label2 = random.randint(0, num_classes - 1)\n        idx2 = random.choice(digit_indices[label2])\n        x2 = x[idx2]\n        pairs.append([x1, x2])\n        labels.append(0)  # aquí 0 = distinta clase\n\n    return np.array(pairs), np.array(labels).astype(\"float32\")\n\npairs_train, labels_train = make_pairs_from_generator(train_generator)\npairs_val, labels_val = make_pairs_from_generator(val_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:39:52.344260Z","iopub.execute_input":"2025-11-26T15:39:52.344627Z","iopub.status.idle":"2025-11-26T15:40:09.752078Z","shell.execute_reply.started":"2025-11-26T15:39:52.344603Z","shell.execute_reply":"2025-11-26T15:40:09.751347Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"history = siamese_model.fit(\n    [pairs_train[:,0], pairs_train[:,1]], labels_train,\n    validation_data=([pairs_val[:,0], pairs_val[:,1]], labels_val),\n    batch_size=BATCH_SIZE,\n    epochs=100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T15:42:57.338920Z","iopub.execute_input":"2025-11-26T15:42:57.339716Z","iopub.status.idle":"2025-11-26T15:43:47.475869Z","shell.execute_reply.started":"2025-11-26T15:42:57.339690Z","shell.execute_reply":"2025-11-26T15:43:47.474900Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 319ms/step - accuracy: 0.5037 - loss: 0.6679 - val_accuracy: 0.5227 - val_loss: 0.6628\nEpoch 2/100\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 205ms/step - accuracy: 0.4992 - loss: 0.6591 - val_accuracy: 0.5227 - val_loss: 0.6604\nEpoch 3/100\n\u001b[1m35/53\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.4868 - loss: 0.6601","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/822549303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = siamese_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairs_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":25}]}