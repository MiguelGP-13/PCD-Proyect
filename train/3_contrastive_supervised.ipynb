{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:22.644477Z",
     "iopub.status.busy": "2025-11-20T10:23:22.643929Z",
     "iopub.status.idle": "2025-11-20T10:23:22.650143Z",
     "shell.execute_reply": "2025-11-20T10:23:22.649436Z",
     "shell.execute_reply.started": "2025-11-20T10:23:22.644453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Add, ReLU, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMG_SIZE = (128, 128)\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 150\n",
    "TEMPERATURE = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:11.193315Z",
     "iopub.status.busy": "2025-11-20T09:32:11.192628Z",
     "iopub.status.idle": "2025-11-20T09:32:16.493117Z",
     "shell.execute_reply": "2025-11-20T09:32:16.492382Z",
     "shell.execute_reply.started": "2025-11-20T09:32:11.193291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta: mel -> 1002 archivos\n",
      "Carpeta: akiec -> 296 archivos\n",
      "Carpeta: bcc -> 461 archivos\n"
     ]
    }
   ],
   "source": [
    "def contar_archivos_en_carpetas(directorio):\n",
    "    # Recorre todas las carpetas dentro del directorio\n",
    "    for carpeta in os.listdir(directorio):\n",
    "        ruta_carpeta = os.path.join(directorio, carpeta)\n",
    "        if os.path.isdir(ruta_carpeta):\n",
    "            # Cuenta solo archivos (no subcarpetas)\n",
    "            archivos = [f for f in os.listdir(ruta_carpeta) \n",
    "                        if os.path.isfile(os.path.join(ruta_carpeta, f))]\n",
    "            print(f\"Carpeta: {carpeta} -> {len(archivos)} archivos\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "directorio_base = \"/kaggle/input/hampreprocessed/malignas_classes/train\"  # Cambia esto por tu ruta\n",
    "contar_archivos_en_carpetas(directorio_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:16.494347Z",
     "iopub.status.busy": "2025-11-20T09:32:16.493993Z",
     "iopub.status.idle": "2025-11-20T09:32:47.378155Z",
     "shell.execute_reply": "2025-11-20T09:32:47.377301Z",
     "shell.execute_reply.started": "2025-11-20T09:32:16.494320Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conjunto: train\n",
      "  Carpeta: benignas -> 7254 archivos\n",
      "  Carpeta: malignas -> 1759 archivos\n",
      "\n",
      "Conjunto: test\n",
      "  Carpeta: benignas -> 807 archivos\n",
      "  Carpeta: malignas -> 195 archivos\n",
      "\n",
      "Suma total por clase (train + test):\n",
      "  benignas -> 8061 archivos\n",
      "  malignas -> 1954 archivos\n"
     ]
    }
   ],
   "source": [
    "def contar_archivos_por_clase(directorio_base):\n",
    "    clases_totales = {}  # acumulador por clase\n",
    "\n",
    "    for conjunto in [\"train\", \"test\"]:\n",
    "        ruta_conjunto = os.path.join(directorio_base, conjunto)\n",
    "        if not os.path.exists(ruta_conjunto):\n",
    "            print(f\"No existe la carpeta: {ruta_conjunto}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nConjunto: {conjunto}\")\n",
    "        for carpeta in os.listdir(ruta_conjunto):\n",
    "            ruta_carpeta = os.path.join(ruta_conjunto, carpeta)\n",
    "            if os.path.isdir(ruta_carpeta):\n",
    "                archivos = [f for f in os.listdir(ruta_carpeta) \n",
    "                            if os.path.isfile(os.path.join(ruta_carpeta, f))]\n",
    "                cantidad = len(archivos)\n",
    "                print(f\"  Carpeta: {carpeta} -> {cantidad} archivos\")\n",
    "\n",
    "                # acumular por clase\n",
    "                if carpeta not in clases_totales:\n",
    "                    clases_totales[carpeta] = 0\n",
    "                clases_totales[carpeta] += cantidad\n",
    "\n",
    "    # Mostrar suma total por clase\n",
    "    print(\"\\nSuma total por clase (train + test):\")\n",
    "    for clase, total in clases_totales.items():\n",
    "        print(f\"  {clase} -> {total} archivos\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "directorio_base = \"/kaggle/input/hampreprocessed/processed\"  # Ruta base que contiene train y test\n",
    "contar_archivos_por_clase(directorio_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:47.379999Z",
     "iopub.status.busy": "2025-11-20T09:32:47.379724Z",
     "iopub.status.idle": "2025-11-20T09:32:47.385266Z",
     "shell.execute_reply": "2025-11-20T09:32:47.384528Z",
     "shell.execute_reply.started": "2025-11-20T09:32:47.379980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/kaggle/input/hampreprocessed/malignas_classes/train\"\n",
    "\n",
    "def get_generators(data_dir, preprocess_fn, target_size=IMG_SIZE, batch_size=BATCH_SIZE, validation_split=0.15):\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_fn,\n",
    "        rotation_range=60,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.12,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        shear_range=0.2,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_split\n",
    "    )\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:47.386330Z",
     "iopub.status.busy": "2025-11-20T09:32:47.386080Z",
     "iopub.status.idle": "2025-11-20T09:32:47.853767Z",
     "shell.execute_reply": "2025-11-20T09:32:47.853214Z",
     "shell.execute_reply.started": "2025-11-20T09:32:47.386307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1496 images belonging to 3 classes.\n",
      "Found 263 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator = get_generators(data_dir, lambda x: x/255.)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "class_names = list(train_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo generador de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:47.854806Z",
     "iopub.status.busy": "2025-11-20T09:32:47.854519Z",
     "iopub.status.idle": "2025-11-20T09:32:47.863418Z",
     "shell.execute_reply": "2025-11-20T09:32:47.862688Z",
     "shell.execute_reply.started": "2025-11-20T09:32:47.854757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def contrastive_encoder(input_shape=(IMG_SIZE[0],IMG_SIZE[0],3), embedding_dim=EMBED_DIM):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Bloque 1\n",
    "    x = Conv2D(64, 3, padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    s = Conv2D(64, 1, padding='same', use_bias=False)(inputs)\n",
    "    s = BatchNormalization()(s)\n",
    "    x = Add()([x, s])\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    # Bloque 2\n",
    "    y = Conv2D(128, 3, padding='same', use_bias=False)(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Conv2D(128, 3, padding='same', use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    s2 = Conv2D(128, 1, padding='same', use_bias=False)(x)\n",
    "    s2 = BatchNormalization()(s2)\n",
    "    y = Add()([y, s2])\n",
    "    y = ReLU()(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    # Bloque 3\n",
    "    z = Conv2D(256, 3, padding='same', use_bias=False)(y)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = ReLU()(z)\n",
    "    z = Conv2D(256, 3, padding='same', use_bias=False)(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    s3 = Conv2D(256, 1, padding='same', use_bias=False)(y)\n",
    "    s3 = BatchNormalization()(s3)\n",
    "    z = Add()([z, s3])\n",
    "    z = ReLU()(z)\n",
    "\n",
    "    z = GlobalAveragePooling2D()(z)\n",
    "    z = Dense(512, activation='relu')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "\n",
    "    # Proyección (cabeza contrastiva)timestamps\n",
    "    p = Dense(EMBED_DIM, activation='relu')(z)\n",
    "    p = Dense(EMBED_DIM)(p)\n",
    "    outputs = Lambda(lambda t: tf.math.l2_normalize(t, axis=1), name=\"proj_norm\")(p)\n",
    "\n",
    "    return Model(inputs, outputs, name=\"ContrastiveEncoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:47.864403Z",
     "iopub.status.busy": "2025-11-20T09:32:47.864100Z",
     "iopub.status.idle": "2025-11-20T09:32:47.881006Z",
     "shell.execute_reply": "2025-11-20T09:32:47.880300Z",
     "shell.execute_reply.started": "2025-11-20T09:32:47.864385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SupConLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, temperature=0.1, name=\"supcon\"):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, y_true, features):\n",
    "        \"\"\"\n",
    "        SupConLoss implementation.\n",
    "        Args:\n",
    "            y_true: [batch] integer class labels (not one-hot).\n",
    "            features: [batch, dim] embeddings.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        features = tf.math.l2_normalize(features, axis=1)\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        # Similarity matrix\n",
    "        sim = tf.matmul(features, features, transpose_b=True)  # [B, B]\n",
    "        sim = sim / self.temperature\n",
    "\n",
    "        # Ensure labels are integers, not one-hot\n",
    "        if y_true.shape.ndims > 1 and y_true.shape[-1] > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "\n",
    "        labels = tf.reshape(y_true, [-1, 1])  # [B, 1]\n",
    "        mask = tf.equal(labels, tf.transpose(labels))  # [B, B]\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        # Remove self-contrast\n",
    "        eye = tf.eye(batch_size, dtype=tf.float32)\n",
    "        logits_mask = tf.ones_like(mask) - eye\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # Log-softmax denominator excluding self\n",
    "        sim_max = tf.reduce_max(sim, axis=1, keepdims=True)\n",
    "        sim = sim - sim_max\n",
    "        exp_sim = tf.exp(sim) * logits_mask\n",
    "        denom = tf.reduce_sum(exp_sim, axis=1, keepdims=True) + 1e-9\n",
    "        log_prob = sim - tf.math.log(denom)\n",
    "\n",
    "        # Average log-prob of positives per anchor\n",
    "        pos_count = tf.reduce_sum(mask, axis=1) + 1e-9\n",
    "        mean_log_pos = tf.reduce_sum(mask * log_prob, axis=1) / pos_count\n",
    "\n",
    "        loss = -tf.reduce_mean(mean_log_pos)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar representaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:32:47.881888Z",
     "iopub.status.busy": "2025-11-20T09:32:47.881647Z",
     "iopub.status.idle": "2025-11-20T09:32:47.899295Z",
     "shell.execute_reply": "2025-11-20T09:32:47.898649Z",
     "shell.execute_reply.started": "2025-11-20T09:32:47.881870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_embeddings(model, generator, k=3, steps=50):\n",
    "    \"\"\"Evalúa embeddings con linear probe o k-NN.\"\"\"\n",
    "    all_embeds, all_labels = [], []\n",
    "    for _ in range(steps):\n",
    "        images, labels = next(generator)\n",
    "        embeds = model(images, training=False).numpy()\n",
    "        all_embeds.append(embeds)\n",
    "        all_labels.append(labels)\n",
    "    X = np.concatenate(all_embeds, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # k-NN\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    return acc\n",
    "\n",
    "def train_supcon(model, train_generator, val_generator, loss_fn, optimizer, epochs=50):\n",
    "    steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "    validation_steps = val_generator.samples // val_generator.batch_size\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_state()\n",
    "        val_loss.reset_state()\n",
    "\n",
    "        # Entrenamiento\n",
    "        for _ in range(steps_per_epoch):\n",
    "            images, labels = next(train_generator)\n",
    "            with tf.GradientTape() as tape:\n",
    "                embeddings = model(images, training=True)\n",
    "                loss = loss_fn(labels, embeddings)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            train_loss.update_state(loss)\n",
    "\n",
    "        # Validación\n",
    "        for _ in range(validation_steps):\n",
    "            images, labels = next(val_generator)\n",
    "            embeddings = model(images, training=False)\n",
    "            loss = loss_fn(labels, embeddings)\n",
    "            val_loss.update_state(loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.result():.4f} - Val Loss: {val_loss.result():.4f}\")\n",
    "\n",
    "        # Evaluación cada 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            acc_knn = evaluate_embeddings(model, val_generator, k=3, steps=validation_steps)\n",
    "            print(f\"k-NN Acc: {acc_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:33:08.226963Z",
     "iopub.status.busy": "2025-11-20T09:33:08.226616Z",
     "iopub.status.idle": "2025-11-20T09:56:39.447302Z",
     "shell.execute_reply": "2025-11-20T09:56:39.446438Z",
     "shell.execute_reply.started": "2025-11-20T09:33:08.226938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763631188.855392      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1763631192.414032      48 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 - Train Loss: 6.4283 - Val Loss: 4.6105\n",
      "Epoch 2/60 - Train Loss: 4.6666 - Val Loss: 4.4532\n",
      "Epoch 3/60 - Train Loss: 4.6144 - Val Loss: 4.4671\n",
      "Epoch 4/60 - Train Loss: 4.6048 - Val Loss: 4.6262\n",
      "Epoch 5/60 - Train Loss: 4.6004 - Val Loss: 4.3693\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 6/60 - Train Loss: 4.5983 - Val Loss: 4.5974\n",
      "Epoch 7/60 - Train Loss: 4.5969 - Val Loss: 4.3624\n",
      "Epoch 8/60 - Train Loss: 4.5970 - Val Loss: 4.3619\n",
      "Epoch 9/60 - Train Loss: 4.5965 - Val Loss: 4.5958\n",
      "Epoch 10/60 - Train Loss: 4.5952 - Val Loss: 4.3616\n",
      "k-NN Acc: 0.9264\n",
      "Epoch 11/60 - Train Loss: 4.5956 - Val Loss: 4.5957\n",
      "Epoch 12/60 - Train Loss: 4.5946 - Val Loss: 4.3618\n",
      "Epoch 13/60 - Train Loss: 4.5948 - Val Loss: 4.3618\n",
      "Epoch 14/60 - Train Loss: 4.5947 - Val Loss: 4.5960\n",
      "Epoch 15/60 - Train Loss: 4.5942 - Val Loss: 4.3619\n",
      "k-NN Acc: 0.9264\n",
      "Epoch 16/60 - Train Loss: 4.5958 - Val Loss: 4.5957\n",
      "Epoch 17/60 - Train Loss: 4.5937 - Val Loss: 4.3619\n",
      "Epoch 18/60 - Train Loss: 4.5941 - Val Loss: 4.3621\n",
      "Epoch 19/60 - Train Loss: 4.5942 - Val Loss: 4.5964\n",
      "Epoch 20/60 - Train Loss: 4.5930 - Val Loss: 4.3624\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 21/60 - Train Loss: 4.5931 - Val Loss: 4.5967\n",
      "Epoch 22/60 - Train Loss: 4.5932 - Val Loss: 4.3626\n",
      "Epoch 23/60 - Train Loss: 4.5934 - Val Loss: 4.3629\n",
      "Epoch 24/60 - Train Loss: 4.5932 - Val Loss: 4.5975\n",
      "Epoch 25/60 - Train Loss: 4.5931 - Val Loss: 4.3630\n",
      "k-NN Acc: 0.9080\n",
      "Epoch 26/60 - Train Loss: 4.5926 - Val Loss: 4.5975\n",
      "Epoch 27/60 - Train Loss: 4.5927 - Val Loss: 4.3626\n",
      "Epoch 28/60 - Train Loss: 4.5926 - Val Loss: 4.3630\n",
      "Epoch 29/60 - Train Loss: 4.5929 - Val Loss: 4.5978\n",
      "Epoch 30/60 - Train Loss: 4.5920 - Val Loss: 4.3642\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 31/60 - Train Loss: 4.5956 - Val Loss: 4.5983\n",
      "Epoch 32/60 - Train Loss: 4.5925 - Val Loss: 4.3644\n",
      "Epoch 33/60 - Train Loss: 4.5929 - Val Loss: 4.3635\n",
      "Epoch 34/60 - Train Loss: 4.5924 - Val Loss: 4.5983\n",
      "Epoch 35/60 - Train Loss: 4.5927 - Val Loss: 4.3633\n",
      "k-NN Acc: 0.9325\n",
      "Epoch 36/60 - Train Loss: 4.5920 - Val Loss: 4.5976\n",
      "Epoch 37/60 - Train Loss: 4.5927 - Val Loss: 4.3636\n",
      "Epoch 38/60 - Train Loss: 4.5922 - Val Loss: 4.3628\n",
      "Epoch 39/60 - Train Loss: 4.5923 - Val Loss: 4.5973\n",
      "Epoch 40/60 - Train Loss: 4.5925 - Val Loss: 4.3636\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 41/60 - Train Loss: 4.5919 - Val Loss: 4.5975\n",
      "Epoch 42/60 - Train Loss: 4.5929 - Val Loss: 4.3626\n",
      "Epoch 43/60 - Train Loss: 4.5923 - Val Loss: 4.3630\n",
      "Epoch 44/60 - Train Loss: 4.5920 - Val Loss: 4.5962\n",
      "Epoch 45/60 - Train Loss: 4.5923 - Val Loss: 4.3625\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 46/60 - Train Loss: 4.5953 - Val Loss: 4.5965\n",
      "Epoch 47/60 - Train Loss: 4.5920 - Val Loss: 4.3625\n",
      "Epoch 48/60 - Train Loss: 4.5921 - Val Loss: 4.3626\n",
      "Epoch 49/60 - Train Loss: 4.5920 - Val Loss: 4.5967\n",
      "Epoch 50/60 - Train Loss: 4.5919 - Val Loss: 4.3625\n",
      "k-NN Acc: 0.9264\n",
      "Epoch 51/60 - Train Loss: 4.5922 - Val Loss: 4.5961\n",
      "Epoch 52/60 - Train Loss: 4.5907 - Val Loss: 4.3633\n",
      "Epoch 53/60 - Train Loss: 4.5919 - Val Loss: 4.3627\n",
      "Epoch 54/60 - Train Loss: 4.5923 - Val Loss: 4.5966\n",
      "Epoch 55/60 - Train Loss: 4.5919 - Val Loss: 4.3624\n",
      "k-NN Acc: 0.9202\n",
      "Epoch 56/60 - Train Loss: 4.5915 - Val Loss: 4.5967\n",
      "Epoch 57/60 - Train Loss: 4.5916 - Val Loss: 4.3631\n",
      "Epoch 58/60 - Train Loss: 4.5925 - Val Loss: 4.3619\n",
      "Epoch 59/60 - Train Loss: 4.5914 - Val Loss: 4.5962\n",
      "Epoch 60/60 - Train Loss: 4.5920 - Val Loss: 4.3628\n",
      "k-NN Acc: 0.9264\n"
     ]
    }
   ],
   "source": [
    "encoder = contrastive_encoder(embedding_dim=EMBED_DIM)\n",
    "encoder.trainable = True\n",
    "loss_fn = SupConLoss(temperature=TEMPERATURE)\n",
    "optimizer = Adam(learning_rate=8e-4)\n",
    "train_supcon(encoder, train_generator, val_generator, loss_fn, optimizer, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:57:00.747368Z",
     "iopub.status.busy": "2025-11-20T09:57:00.746717Z",
     "iopub.status.idle": "2025-11-20T09:57:00.849804Z",
     "shell.execute_reply": "2025-11-20T09:57:00.849225Z",
     "shell.execute_reply.started": "2025-11-20T09:57:00.747343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Current timestamp\n",
    "timestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n",
    "\n",
    "encoder.save(f\"encoder_{timestamp}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular centroides de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:37:27.500265Z",
     "iopub.status.busy": "2025-11-20T10:37:27.499678Z",
     "iopub.status.idle": "2025-11-20T10:37:27.507483Z",
     "shell.execute_reply": "2025-11-20T10:37:27.506588Z",
     "shell.execute_reply.started": "2025-11-20T10:37:27.500238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_centroids(encoder, generator):\n",
    "    \"\"\"\n",
    "    Calcula centroides de clase a partir de un generator de Keras.\n",
    "    Devuelve un dict {class_index: centroid_vector}.\n",
    "    \"\"\"\n",
    "    embeds, labels = [], []\n",
    "    for i in range(len(generator)):\n",
    "        x_batch, y_batch = generator[i]\n",
    "        e = encoder.predict(x_batch, verbose=0)\n",
    "        e = normalize(e)  # normalizar embeddings fila a fila\n",
    "        embeds.append(e)\n",
    "        labels.append(np.argmax(y_batch, axis=1))  # convertir one-hot a entero\n",
    "    # print(pd.Series(labels).value_counts())\n",
    "\n",
    "    embeds = np.concatenate(embeds)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    centroids = {}\n",
    "    for c in np.unique(labels):\n",
    "        class_embeds = embeds[labels == c]\n",
    "        centroid = class_embeds.mean(axis=0)\n",
    "        centroid = centroid / np.linalg.norm(centroid)  # normalizar centroide\n",
    "        centroids[int(c)] = centroid.tolist()  # convertir a lista para JSON\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def save_centroids(centroids, filename=None):\n",
    "    if not filename:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n",
    "        filename = f\"centroids_{timestamp}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(centroids, f)\n",
    "\n",
    "def load_centroids(filename=\"centroids.json\"):\n",
    "    with open(filename, \"r\") as f:\n",
    "        centroids = json.load(f)\n",
    "    # convertir a numpy arrays\n",
    "    centroids = {int(k): np.array(v) for k, v in centroids.items()}\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:37:27.659443Z",
     "iopub.status.busy": "2025-11-20T10:37:27.658957Z",
     "iopub.status.idle": "2025-11-20T10:37:44.592674Z",
     "shell.execute_reply": "2025-11-20T10:37:44.592059Z",
     "shell.execute_reply.started": "2025-11-20T10:37:27.659421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "centroids = compute_centroids(encoder, train_generator)\n",
    "save_centroids(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:32:28.860078Z",
     "iopub.status.busy": "2025-11-20T10:32:28.859482Z",
     "iopub.status.idle": "2025-11-20T10:32:28.866435Z",
     "shell.execute_reply": "2025-11-20T10:32:28.865808Z",
     "shell.execute_reply.started": "2025-11-20T10:32:28.860053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_class(encoder, x, centroids, probs=False):\n",
    "    \"\"\"\n",
    "    Predice la clase de una sola imagen x usando centroides.\n",
    "    \"\"\"\n",
    "    e = encoder.predict(np.expand_dims(x, axis=0), verbose=0)\n",
    "    e = normalize(e)  # normalizar embedding\n",
    "    sims = {c: np.dot(e, centroids[c]) for c in centroids}\n",
    "    if probs:\n",
    "        return sims\n",
    "    return max(sims, key=sims.get)  # clase con mayor similitud\n",
    "\n",
    "def evaluate_accuracy(encoder, val_generator, centroids):\n",
    "    \"\"\"\n",
    "    Calcula el accuracy del val_generator usando centroides.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for i in range(len(val_generator)):\n",
    "        x_batch, y_batch = val_generator[i]\n",
    "        labels = np.argmax(y_batch, axis=1)  # convertir one-hot a enteros\n",
    "\n",
    "        for j in range(len(x_batch)):\n",
    "            pred = predict_class(encoder, x_batch[j], centroids)\n",
    "            y_true.append(labels[j])\n",
    "            y_pred.append(pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy en val_generator: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:32:29.972976Z",
     "iopub.status.busy": "2025-11-20T10:32:29.972605Z",
     "iopub.status.idle": "2025-11-20T10:34:23.960322Z",
     "shell.execute_reply": "2025-11-20T10:34:23.959689Z",
     "shell.execute_reply.started": "2025-11-20T10:32:29.972950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en val_generator: 0.4398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43983957219251335"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(encoder, train_generator, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:29:37.920719Z",
     "iopub.status.busy": "2025-11-20T10:29:37.919977Z",
     "iopub.status.idle": "2025-11-20T10:30:24.679953Z",
     "shell.execute_reply": "2025-11-20T10:30:24.679135Z",
     "shell.execute_reply.started": "2025-11-20T10:29:37.920697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9357126515671471"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_embeddings(encoder, val_generator, k=3, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:45:04.330641Z",
     "iopub.status.busy": "2025-11-20T10:45:04.330051Z",
     "iopub.status.idle": "2025-11-20T10:45:04.337737Z",
     "shell.execute_reply": "2025-11-20T10:45:04.337091Z",
     "shell.execute_reply.started": "2025-11-20T10:45:04.330611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def train_knn(encoder, train_generator, k=10):\n",
    "    \"\"\"\n",
    "    Entrena un KNN sobre los embeddings del train_generator.\n",
    "    Devuelve el clasificador entrenado.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(train_generator)):\n",
    "        x_batch, y_batch = train_generator[i]\n",
    "        e = encoder.predict(x_batch, verbose=0)\n",
    "        e = e / np.linalg.norm(e, axis=1, keepdims=True)  # normalizar embeddings\n",
    "        X.append(e)\n",
    "        y.append(np.argmax(y_batch, axis=1))\n",
    "\n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=\"cosine\")\n",
    "    knn.fit(X, y)\n",
    "    return knn\n",
    "\n",
    "def evaluate_knn(encoder, val_generator, knn):\n",
    "    \"\"\"\n",
    "    Evalúa un KNN entrenado sobre el val_generator.\n",
    "    \"\"\"\n",
    "    X_val, y_val = [], []\n",
    "    for i in range(len(val_generator)):\n",
    "        x_batch, y_batch = val_generator[i]\n",
    "        e = encoder.predict(x_batch, verbose=0)\n",
    "        e = e / np.linalg.norm(e, axis=1, keepdims=True)\n",
    "        X_val.append(e)\n",
    "        y_val.append(np.argmax(y_batch, axis=1))\n",
    "\n",
    "    X_val = np.concatenate(X_val)\n",
    "    y_val = np.concatenate(y_val)\n",
    "\n",
    "    y_pred = knn.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Accuracy en val_generator con KNN: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:45:05.369287Z",
     "iopub.status.busy": "2025-11-20T10:45:05.368688Z",
     "iopub.status.idle": "2025-11-20T10:45:25.296702Z",
     "shell.execute_reply": "2025-11-20T10:45:25.295936Z",
     "shell.execute_reply.started": "2025-11-20T10:45:05.369262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenado\n",
      "Accuracy en val_generator con KNN: 0.4867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4866920152091255"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = train_knn(encoder, train_generator)\n",
    "print(\"Entrenado\")\n",
    "evaluate_knn(encoder, val_generator, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:59:30.774518Z",
     "iopub.status.busy": "2025-11-20T09:59:30.774260Z",
     "iopub.status.idle": "2025-11-20T09:59:30.794684Z",
     "shell.execute_reply": "2025-11-20T09:59:30.793919Z",
     "shell.execute_reply.started": "2025-11-20T09:59:30.774498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoder.trainable = False\n",
    "# x = encoder.output\n",
    "# clf = Dense(num_classes, activation=\"softmax\")(x)\n",
    "# classifier = Model(encoder.input, clf)\n",
    "# classifier.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:17:52.230284Z",
     "iopub.status.busy": "2025-11-20T10:17:52.226015Z",
     "iopub.status.idle": "2025-11-20T10:17:52.233694Z",
     "shell.execute_reply": "2025-11-20T10:17:52.233003Z",
     "shell.execute_reply.started": "2025-11-20T10:17:52.230259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# labels = train_generator.classes\n",
    "# print(pd.Series(labels).value_counts())\n",
    "# print(num_classes)\n",
    "# class_weights = dict(enumerate(compute_class_weight(\n",
    "#     class_weight=\"balanced\",\n",
    "#     classes=np.unique(labels),\n",
    "#     y=labels\n",
    "# )))\n",
    "\n",
    "# classifier.fit(\n",
    "#     train_generator,\n",
    "#     validation_data=val_generator,\n",
    "#     epochs=100,\n",
    "#     class_weight=class_weights\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:17:57.429916Z",
     "iopub.status.busy": "2025-11-20T10:17:57.429325Z",
     "iopub.status.idle": "2025-11-20T10:17:57.432968Z",
     "shell.execute_reply": "2025-11-20T10:17:57.432188Z",
     "shell.execute_reply.started": "2025-11-20T10:17:57.429890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Current timestamp\n",
    "# timestamp = datetime.datetime.now().strftime(\"%m_%d_h%H_%M\")\n",
    "\n",
    "# classifier.save(f\"classifier_{timestamp}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T09:56:39.469494Z",
     "iopub.status.idle": "2025-11-20T09:56:39.470376Z",
     "shell.execute_reply": "2025-11-20T09:56:39.470192Z",
     "shell.execute_reply.started": "2025-11-20T09:56:39.470175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_knn(model, val_generator, class_names, k=5):\n",
    "    embs, labs = [], []\n",
    "    # recorrer todo el generador de validación\n",
    "    for imgs, labels in val_generator:\n",
    "        e = model(imgs, training=False).numpy()\n",
    "        embs.append(e)\n",
    "        # si labels es one-hot, convertir a entero con argmax\n",
    "        if labels.ndim > 1:\n",
    "            labs.append(np.argmax(labels, axis=1))\n",
    "        else:\n",
    "            labs.append(labels)\n",
    "\n",
    "    # concatenar embeddings y etiquetas\n",
    "    X = np.concatenate(embs, axis=0)\n",
    "    y = np.concatenate(labs, axis=0)\n",
    "\n",
    "    # entrenar y evaluar KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
    "    knn.fit(X, y)\n",
    "    y_pred = knn.predict(X)\n",
    "\n",
    "    print(classification_report(y, y_pred, target_names=class_names))\n",
    "    print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T09:56:39.471487Z",
     "iopub.status.idle": "2025-11-20T09:56:39.471988Z",
     "shell.execute_reply": "2025-11-20T09:56:39.471878Z",
     "shell.execute_reply.started": "2025-11-20T09:56:39.471866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_knn(encoder, val_generator, class_names, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T09:56:39.472932Z",
     "iopub.status.idle": "2025-11-20T09:56:39.473338Z",
     "shell.execute_reply": "2025-11-20T09:56:39.473186Z",
     "shell.execute_reply.started": "2025-11-20T09:56:39.473170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings_3d(model, val_ds, class_names, method=\"tsne\"):\n",
    "    # 1. Extraer embeddings y etiquetas\n",
    "    embs, labs = [], []\n",
    "    for imgs, labels in val_ds:\n",
    "        e = model(imgs, training=False).numpy()\n",
    "        embs.append(e)\n",
    "        labs.append(labels.numpy())\n",
    "    X = np.concatenate(embs, axis=0)\n",
    "    y = np.concatenate(labs, axis=0)\n",
    "\n",
    "    # 2. Reducir a 3D\n",
    "    if method == \"tsne\":\n",
    "        reducer = TSNE(n_components=3, perplexity=30, learning_rate=200, random_state=42)\n",
    "    else:\n",
    "        reducer = PCA(n_components=3)\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "\n",
    "    # 3. Visualizar con Plotly\n",
    "    fig = px.scatter_3d(\n",
    "        x=X_reduced[:,0], y=X_reduced[:,1], z=X_reduced[:,2],\n",
    "        color=[class_names[i] for i in y],\n",
    "        title=f\"Embeddings en 3D ({method.upper()})\",\n",
    "        opacity=0.7\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-20T09:56:39.474235Z",
     "iopub.status.idle": "2025-11-20T09:56:39.474666Z",
     "shell.execute_reply": "2025-11-20T09:56:39.474485Z",
     "shell.execute_reply.started": "2025-11-20T09:56:39.474466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_embeddings_3d(encoder, val_generator, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8656484,
     "sourceId": 13692377,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
